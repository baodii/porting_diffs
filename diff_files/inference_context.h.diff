--- generated_code/csrc/transformer/inference/includes/inference_context.h	2023-06-08 22:43:13.717967801 -0700
+++ manual_fixed/csrc/transformer/inference/includes/inference_context.h	2023-06-08 22:43:29.114099828 -0700
@@ -5,15 +5,17 @@
 
 #pragma once
 
-#include <sycl/sycl.hpp>
-#include <dpct/dpct.hpp>
-#include <c10/cuda/CUDAStream.h>
+
+#include <ATen/record_function.h>
+#include <c10/core/Stream.h>
+#include <ipex.h>
+#include <torch/extension.h>
+#include <torch/library.h>
 #include <cassert>
 #include <iostream>
+#include <oneapi/mkl.hpp>
+#include <oneapi/mkl/rng/device.hpp>
 #include <vector>
-#include <dpct/blas_utils.hpp>
-
-#include <cmath>
 
 #define MEGABYTE (1024 * 1024)
 #define GIGABYTE (1024 * 1024 * 1024)
@@ -60,15 +62,21 @@
           _attention_unfused_workspace_offset(0),
           _workSpaceSize(0)
     {
+        auto type_ = c10::DeviceType::XPU;
+        c10::impl::VirtualGuardImpl impl(type_);
+        auto device_ = c10::Device(type_);
+        c10::Stream stream = impl.getStream(device_);
+        _gen = new oneapi::mkl::rng::philox4x32x10(xpu::get_queue_from_stream(stream), 123);
+        
         _workSpaceSize = 0;
         _workspace = 0;
-        if (cublasCreate(&_cublasHandle) != CUBLAS_STATUS_SUCCESS) {
-            auto message = std::string("Fail to create cublas handle.");
+        if ((_onemklQ = xpu::get_queue_from_stream(stream), 0) != 0) {
+            auto message = std::string("Fail to create onemkl handle.");
             std::cerr << message << std::endl;
             throw std::runtime_error(message);
         }
 #ifndef __HIP_PLATFORM_HCC__
-        cublasSetMathMode(_cublasHandle, CUBLAS_TENSOR_OP_MATH);
+        /* cublasSetMathMode(_cublasHandle, CUBLAS_TENSOR_OP_MATH); */
 #endif
         _comp1_event = new sycl::event();
         _comp2_event = new sycl::event();
@@ -78,12 +86,14 @@
 
     virtual ~InferenceContext()
     {
-        cublasDestroy(_cublasHandle);
-        sycl::free(_workspace, dpct::get_default_queue());
-        dpct::destroy_event(_comp1_event);
-        dpct::destroy_event(_comp2_event);
-        dpct::destroy_event(_comp_event);
-        dpct::destroy_event(_comm_event);
+        /* cublasDestroy(_cublasHandle); */
+        free(_gen);
+        
+        auto type_ = c10::DeviceType::XPU;
+        c10::impl::VirtualGuardImpl impl(type_);
+        auto device_ = c10::Device(type_);
+        c10::Stream stream = impl.getStream(device_);
+        sycl::free(_workspace, xpu::get_queue_from_stream(stream));
     }
 
     static InferenceContext& Instance()
@@ -126,19 +136,20 @@
         size_t temp_size = batch_size * (num_heads / mp_size) * max_out_tokens;
         size_t cache_size =
             num_layers * batch_size * ((num_heads * effective_head_size) / mp_size) * 2;
-        size_t minimal_requirements =
-            temp_size + (_free_memory_size > GIGABYTE ? 500 : 100) * MEGABYTE;
-        if (_free_memory_size < minimal_requirements) {
-            printf("Requested:\t%lu\nFree:\t%lu\nTotal:\t%lu\n",
-                   minimal_requirements,
-                   _free_memory_size,
-                   total_size);
-            throw std::runtime_error("Workspace can't be allocated, no enough memory.");
-        }
-
-        _max_seq_len = ((_free_memory_size - minimal_requirements) / elem_size) /
-                       (activation_size + temp_size + cache_size);
-        _max_seq_len = std::min((size_t)max_out_tokens, _max_seq_len);
+        /* size_t minimal_requirements = */
+        /*     temp_size + (_free_memory_size > GIGABYTE ? 500 : 100) * MEGABYTE; */
+        /* if (_free_memory_size < minimal_requirements) { */
+        /*     printf("Requested:\t%lu\nFree:\t%lu\nTotal:\t%lu\n", */
+        /*            minimal_requirements, */
+        /*            _free_memory_size, */
+        /*            total_size); */
+        /*     throw std::runtime_error("Workspace can't be allocated, no enough memory."); */
+        /* } */
+
+        /* _max_seq_len = ((_free_memory_size - minimal_requirements) / elem_size) / */
+        /*                (activation_size + temp_size + cache_size); */
+        /* _max_seq_len = std::min((size_t)max_out_tokens, _max_seq_len); */
+        _max_seq_len = (size_t)max_out_tokens;
         size_t workSpaceSize = ((external_cache ? (activation_size + temp_size)
                                                 : (activation_size + temp_size + cache_size))) *
                                _max_seq_len * elem_size;
@@ -211,22 +222,28 @@
 
     inline void advance_tokens() { _num_tokens++; }
 
-    dpct::queue_ptr GetCommStream(bool async_op = false)
-    {
-        if (!_comm_stream)
-            _comm_stream = async_op ? at::cuda::getStreamFromPool(true)
-                                    : at::cuda::getCurrentCUDAStream();
-        return _comm_stream;
-    }
+    /* dpct::queue_ptr GetCommStream(bool async_op = false) */
+    /* { */
+    /*     if (!_comm_stream) */
+    /*         _comm_stream = async_op ? at::cuda::getStreamFromPool(true) */
+    /*                                 : at::cuda::getCurrentCUDAStream(); */
+    /*     return _comm_stream; */
+    /* } */
     dpct::queue_ptr GetCurrentStream(bool other_stream = false)
     {
+        auto type_ = c10::DeviceType::XPU;
+        c10::impl::VirtualGuardImpl impl(type_);
+        auto device_ = c10::Device(type_);
+        c10::Stream stream = impl.getStream(device_);
+        return &xpu::get_queue_from_stream(stream);
         // get current pytorch stream.
-        if (other_stream) {
-            if (!_stream) _stream = at::cuda::getStreamFromPool(true);
-            return _stream;
-        }
-        dpct::queue_ptr stream = at::cuda::getCurrentCUDAStream();
-        return stream;
+        
+        /* if (other_stream) { */
+        /*     if (!_stream) _stream = at::cuda::getStreamFromPool(true); */
+        /*     return _stream; */
+        /* } */
+        /* dpct::queue_ptr stream = &dpct::get_default_queue(); */
+        /* return stream; */
     }
 
     void release_workspace()
@@ -240,7 +257,8 @@
         _workspace = (void*)sycl::malloc_device(_workSpaceSize, dpct::get_default_queue());
         return _workspace != nullptr;
     }
-    dpct::queue_ptr GetCublasHandle() { return _cublasHandle; }
+    /* dpct::queue_ptr GetCublasHandle() { return _cublasHandle; } */
+    sycl::queue GetOneMKLQ() { return _onemklQ; }
 
     std::pair<uint64_t, uint64_t> IncrementOffset(uint64_t offset_inc)
     {
@@ -253,19 +271,21 @@
 
     const std::vector<std::array<int, 3>>& GetGemmAlgos() const { return _gemm_algos; }
 
-    inline void SynchComp()
-    {
-        cudaEventRecord(_comp_event, _comp_stream);
-        cudaStreamWaitEvent(_comm_stream, _comp_event, 0);
-    }
-    inline void SynchComm()
-    {
-        cudaEventRecord(_comm_event, _comm_stream);
-        cudaStreamWaitEvent(_comp_stream, _comm_event, 0);
-    }
+    /* inline void SynchComp() */
+    /* { */
+    /*     cudaEventRecord(_comp_event, _comp_stream); */
+    /*     cudaStreamWaitEvent(_comm_stream, _comp_event, 0); */
+    /* } */
+    /* inline void SynchComm() */
+    /* { */
+    /*     cudaEventRecord(_comm_event, _comm_stream); */
+    /*     cudaStreamWaitEvent(_comp_stream, _comm_event, 0); */
+    /* } */
 
 private:
-    cublasHandle_t _cublasHandle;
+    /* cublasHandle_t _cublasHandle; */
+    oneapi::mkl::rng::philox4x32x10* _gen;
+    sycl::queue _onemklQ;
 
     dpct::event_ptr _comp_event;
     dpct::event_ptr _comm_event;
